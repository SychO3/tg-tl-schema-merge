#!/usr/bin/env python3
"""
Fetch and merge the latest Telegram TL schema from two sources, and append a changelog
whenever the merged output changes.

Outputs:
- merged.tl: merged result

Other details are same as previous version.
"""

import argparse
import datetime as dt
import json
import os
import sys
from dataclasses import dataclass
from pathlib import Path
from typing import Optional, Tuple, List
from urllib.parse import urlencode
from urllib.request import Request, urlopen
from urllib.error import HTTPError, URLError
import difflib

GITHUB_API = "https://api.github.com"
RAW_BASE = "https://raw.githubusercontent.com"

SOURCES = [
    dict(owner="telegramdesktop", repo="tdesktop",
         path="Telegram/SourceFiles/mtproto/scheme/api.tl", ref="dev"),
    dict(owner="tdlib", repo="td",
         path="td/generate/scheme/telegram_api.tl", ref="master"),
]


@dataclass
class FileInfo:
    owner: str
    repo: str
    path: str
    ref: str
    sha: str
    commit_date_iso: str
    size: int
    etag: Optional[str] = None
    filename: Optional[str] = None


def github_headers() -> dict:
    headers = {
        "Accept": "application/vnd.github+json",
        "User-Agent": "fetch-and-merge-tl/1.2",
    }
    token = os.environ.get("GITHUB_TOKEN")
    if token:
        headers["Authorization"] = f"Bearer {token}"
    return headers


def http_get_json(url: str, headers: dict) -> dict:
    req = Request(url, headers=headers)
    with urlopen(req, timeout=30) as resp:
        charset = resp.headers.get_content_charset() or "utf-8"
        data = resp.read().decode(charset, errors="replace")
        return json.loads(data)


def http_get_bytes(url: str, headers: dict) -> Tuple[bytes, dict]:
    req = Request(url, headers=headers)
    with urlopen(req, timeout=60) as resp:
        return resp.read(), dict(resp.headers.items())


def latest_commit_for_path(owner: str, repo: str, path: str, ref: str) -> Tuple[str, str]:
    params = urlencode({"path": path, "sha": ref, "per_page": 1})
    url = f"{GITHUB_API}/repos/{owner}/{repo}/commits?{params}"
    data = http_get_json(url, github_headers())
    if not data:
        raise RuntimeError(f"No commits found for {owner}/{repo}:{path}@{ref}")
    commit = data[0]
    sha = commit["sha"]
    date_str = commit["commit"]["committer"]["date"] or commit["commit"]["author"]["date"]
    date_iso = date_str if date_str.endswith("Z") else dt.datetime.fromisoformat(date_str.replace("Z","+00:00")).astimezone(dt.timezone.utc).isoformat().replace("+00:00","Z")
    return sha, date_iso


def download_raw(owner: str, repo: str, path: str, sha: str) -> Tuple[bytes, Optional[str]]:
    raw_url = f"{RAW_BASE}/{owner}/{repo}/{sha}/{path}"
    content, headers = http_get_bytes(raw_url, {"User-Agent": "fetch-and-merge-tl/1.2"})
    return content, headers.get("ETag")


def write_bytes(p: Path, data: bytes) -> None:
    p.parent.mkdir(parents=True, exist_ok=True)
    with open(p, "wb") as f:
        f.write(data)


def merge_tl(newer_lines: List[str], older_lines: List[str]) -> List[str]:
    """Keep newer_lines order; append lines from older_lines not present verbatim."""
    seen = set(newer_lines)
    merged = list(newer_lines)
    for line in older_lines:
        if line not in seen:
            merged.append(line)
            seen.add(line)
    return merged


def header_block(infos: List[FileInfo]) -> str:
    lines = [
        "// === merged.tl generated by fetch-and-merge-tl ===",
    ]
    for i in infos:
        lines.append(f"// source: {i.owner}/{i.repo}/{i.path}@{i.sha[:7]}")
    lines.append("// merge strategy: td (primary) order retained; unique lines from tdesktop appended; exact-line de-dup.")
    lines.append("")
    return "\n".join(lines)


def diff_stats(old: str, new: str) -> Tuple[int, int]:
    added = removed = 0
    for tag in difflib.ndiff(old.splitlines(True), new.splitlines(True)):
        if tag.startswith("+ "):
            added += 1
        elif tag.startswith("- "):
            removed += 1
    return added, removed


def short_unified_diff(old: str, new: str, n_context: int = 3, max_lines: int = 120) -> str:
    diff = list(difflib.unified_diff(
        old.splitlines(True), new.splitlines(True),
        fromfile="merged.old", tofile="merged.new", n=n_context
    ))
    if len(diff) > max_lines:
        diff = diff[:max_lines] + ["... (diff truncated)\n"]
    return "".join(diff)


def main(argv=None) -> int:
    parser = argparse.ArgumentParser(description="Fetch and merge TL schemas (td primary), and append changelog on updates.")
    parser.add_argument("--outdir", default="schemas", help="Output directory (default: ./schemas)")
    args = parser.parse_args(argv)

    outdir = Path(args.outdir)
    outdir.mkdir(parents=True, exist_ok=True)

    infos: List[FileInfo] = []

    # Fetch both sources
    for src in SOURCES:
        owner, repo, path, ref = src["owner"], src["repo"], src["path"], src["ref"]
        sha, date_iso = latest_commit_for_path(owner, repo, path, ref)
        content, etag = download_raw(owner, repo, path, sha)
        fname = f"{repo}-{sha[:7]}.tl"
        write_bytes(outdir / fname, content)

        infos.append(FileInfo(
            owner=owner, repo=repo, path=path, ref=ref, sha=sha,
            commit_date_iso=date_iso, size=len(content), etag=etag, filename=fname
        ))
        print(f"[OK] saved {fname} ({len(content)} bytes)")

    # Determine primary (td) and supplementary (tdesktop)
    primary_idx = next((i for i, info in enumerate(infos) if info.repo == "td"), 0)
    primary = infos[primary_idx]
    supplement = infos[1 - primary_idx]

    # latest.tl (now represents the primary td snapshot)
    write_bytes(outdir / "latest.tl", (outdir / primary.filename).read_bytes())

    # Merge raw text
    primary_text = (outdir / primary.filename).read_text(encoding="utf-8", errors="replace").splitlines(True)
    supplement_text = (outdir / supplement.filename).read_text(encoding="utf-8", errors="replace").splitlines(True)
    merged_lines = merge_tl(primary_text, supplement_text)

    # Add header
    merged_lines = [header_block([primary, supplement])] + merged_lines

    merged_text = "".join(merged_lines)

    # Write merged.tl if changed and append CHANGELOG
    merged_path = outdir / "merged.tl"
    old_text = merged_path.read_text(encoding="utf-8", errors="replace") if merged_path.exists() else ""

    if merged_text != old_text:
        write_bytes(merged_path, merged_text.encode("utf-8"))
        added, removed = diff_stats(old_text, merged_text)
        with open(outdir / "CHANGELOG.txt", "a", encoding="utf-8") as log:
            ts = dt.datetime.now(dt.timezone.utc).isoformat().replace("+00:00","Z")
            log.write(f"[{ts}] merged.tl updated\n")
            log.write(f" primary={primary.repo}@{primary.sha[:7]} ({primary.commit_date_iso})  supplement={supplement.repo}@{supplement.sha[:7]} ({supplement.commit_date_iso})  +{added} -{removed}\n")
            log.write(short_unified_diff(old_text, merged_text) + "\n")
        print(f"[OK] merged.tl updated; CHANGELOG.txt appended")
    else:
        print("[OK] merged.tl unchanged (no changelog entry)")

    # metadata
    meta = {
        "generated_at_utc": dt.datetime.now(dt.timezone.utc).isoformat().replace("+00:00","Z"),
        "latest_index": primary_idx,
        "entries": [dict(owner=i.owner, repo=i.repo, path=i.path, ref=i.ref, sha=i.sha, commit_date=i.commit_date_iso, size=i.size, etag=i.etag, file=i.filename) for i in infos]
    }
    write_bytes(outdir / "metadata.json", json.dumps(meta, indent=2, ensure_ascii=False).encode("utf-8"))
    print("[OK] metadata.json written")
    return 0


if __name__ == "__main__":
    try:
        sys.exit(main())
    except HTTPError as e:
        print(f"[HTTPError] {e}", file=sys.stderr); sys.exit(2)
    except URLError as e:
        print(f"[URLError] {e}", file=sys.stderr); sys.exit(2)
    except Exception as e:
        print(f"[Error] {e}", file=sys.stderr); sys.exit(1)
